# Sprint 8 

Nesta sprint, exploramos ferramentas e tecnologias para trabalhar com grandes volumes de dados e realizar transformações eficientes. Os principais tópicos abordados foram:

- Introdução ao PySpark para manipulação de dados.
- Criação e manipulação de arquivos de grande escala.
- Uso do AWS Glue para integração de dados e execução de jobs.
- Configuração de Crawlers no Athena para consulta eficiente de dados.

Nosso objetivo foi aprender as bases de processamento distribuído e aplicar conceitos em um desafio prático.

## Ferramentas Utilizadas
- **Python**: Para manipulação e geração de dados.
- **PySpark**: Para transformação e análise de grandes volumes de dados.
- **AWS Glue**: Para criação de jobs ETL.
- **Amazon Athena**: Para consultas SQL em dados no S3.

[Exercicios](../Exercicios/) 
[Desafio](../Desafio/)
[Evidencias](../Evidencias/)
